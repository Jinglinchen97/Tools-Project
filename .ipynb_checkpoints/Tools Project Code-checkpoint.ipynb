{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Web Crawling for Stock Related News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we combine all these into a def function\n",
    "# stock_name should be 'AAPL' & 'GOOG' cannot be 'apple' or 'google'\n",
    "def get_article_link(stock_name):\n",
    "    all_links = []\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    url = \"https://finance.yahoo.com/quote/\"+ str(stock_name) +\"/?p=\" + str(stock_name)\n",
    "    #send the request and get the response\n",
    "    response = requests.get(url)\n",
    "    if not response.status_code == 200:\n",
    "        return None\n",
    "    try:\n",
    "        results_page = BeautifulSoup(response.content,'lxml')\n",
    "        # we find the href contains in the tag_h3\n",
    "        all_h3_tags = results_page.find_all('h3', {'class': \"Mb(5px)\"})\n",
    "        article_link = []\n",
    "        # try to find the href in the tag_h3\n",
    "        for tag in all_h3_tags:\n",
    "            try:\n",
    "                article_link.append(tag.find('a').get('href'))\n",
    "            except:\n",
    "                return None\n",
    "        for link in article_link:\n",
    "            if 'http' in link:\n",
    "                all_links.append(url)\n",
    "            else:\n",
    "                home_url = 'https://finance.yahoo.com'\n",
    "                url = home_url + link\n",
    "                all_links.append(url)\n",
    "        return all_links\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = 'AAPL'\n",
    "get_article_link(stock_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have already find the link of certain stock article\n",
    "# so we have to get the article content and artcile issued data through that link\n",
    "# we firstly use one link and then combines them into a def function\n",
    "get_article_link(stock_name)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "if not response.status_code == 200:\n",
    "    print('there is something wrong with link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_page = BeautifulSoup(response.content,'lxml')\n",
    "result_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we have find time is hidden in the tag_time from the result_page\n",
    "time_tag = result_page.find('time')\n",
    "print(time_tag)\n",
    "time = time_tag.get_text()\n",
    "time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after we get datetime, we have to get the article content which we use it to do sentiment analysis\n",
    "content_tag = result_page.find_all('p',{'class':'canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm'})\n",
    "articles = []\n",
    "for content in content_tag:\n",
    "    article = content.get_text()\n",
    "    articles.append(article)\n",
    "print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we combine the time and article into a list and construct a list\n",
    "def get_article_content(all_links):\n",
    "    article_content = []\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    for link in all_links:\n",
    "        contents = []\n",
    "        response = requests.get(link)\n",
    "        if not response.status_code == 200:\n",
    "            return None\n",
    "        result_page = BeautifulSoup(response.content,'lxml')\n",
    "        time_tag = result_page.find('time')\n",
    "        time = time_tag.get_text()\n",
    "        contents.append(time)\n",
    "        content_tag = result_page.find_all('p',{'class':'canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm'})\n",
    "        for content in content_tag:\n",
    "            article = content.get_text()\n",
    "            contents.append(article)\n",
    "        article_content.append(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scrolling Down Page to Get More New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /usr/local/lib/python3.6/site-packages (18.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /usr/local/lib/python3.6/site-packages (3.141.0)\r\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/site-packages (from selenium) (1.24.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys  \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrolling_down_page(stock_name):\n",
    "    #Using chromedriver to open the web\n",
    "    url = \"https://finance.yahoo.com/quote/\"+ str(stock_name) +\"/?p=\" + str(stock_name)\n",
    "    driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "    time.sleep(3) \n",
    "    \n",
    "    #Selenium automates browsers, scrolling down the page until contents are fully updated\n",
    "    from selenium.webdriver import ActionChains\n",
    "    for i in range(1000): \n",
    "        ActionChains(driver).key_down(Keys.DOWN).perform() \n",
    "        print(f'have finished {i} times')\n",
    "        time.sleep(1)\n",
    "    \n",
    "    #Acquire page source code\n",
    "    html_ = driver.page_source.encode('utf-8')\n",
    "    results_page = BeautifulSoup(html_,'lxml')\n",
    "    \n",
    "    return results_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change artical content format for text mining\n",
    "def artical_format(artical_content):\n",
    "    artical_texts = []\n",
    "    for i in artical_content:\n",
    "        if len(i)>1:\n",
    "            artical_texts.append([i[0]]+[''.join(i[1:])])\n",
    "    return artical_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return text analysis results\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk import sent_tokenize\n",
    "import pandas as pd\n",
    "def vader_comparison(texts):\n",
    "    headers = ['Name','pos','neg','neu','compound']\n",
    "    result=pd.DataFrame(columns=headers)\n",
    "    print(\"Name\\t\",'  pos\\t','neg\\t','neu\\t','compound')\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for i in range(len(texts)):\n",
    "        name = texts[i][0]\n",
    "        sentences = sent_tokenize(texts[i][1])\n",
    "        pos=compound=neu=neg=0\n",
    "        for sentence in sentences:\n",
    "            vs = analyzer.polarity_scores(sentence)\n",
    "            pos+=vs['pos']/(len(sentences))\n",
    "            neu+=vs['neu']/(len(sentences))\n",
    "            neg+=vs['neg']/(len(sentences))\n",
    "            compound+=vs['compound']/(len(sentences))\n",
    "        result=result.append(pd.DataFrame([[name,pos,neg,neu,compound]],columns=headers))\n",
    "        print('%-10s'%name,'%1.2f\\t'%pos,'%1.2f\\t'%neg,'%1.2f\\t'%neu,'%1.2f\\t'%compound)\n",
    "    result=result.sort_values(by='Name')\n",
    "    result.index=range(len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = 'AAPL'\n",
    "all_links=get_article_link(stock_name)\n",
    "artical_content=get_article_content(all_links)\n",
    "artical_texts=artical_format(artical_content)\n",
    "news_result=vader_comparison(artical_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Acquiring Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/site-packages (0.23.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/site-packages (from pandas) (2.7.3)\r\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/site-packages (from pandas) (1.15.4)\r\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/site-packages (from pandas) (2018.7)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas_datareader in /usr/local/lib/python3.6/site-packages (0.7.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/site-packages (from pandas_datareader) (1.10.11)\n",
      "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/site-packages (from pandas_datareader) (2.20.1)\n",
      "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.6/site-packages (from pandas_datareader) (0.23.4)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.6/site-packages (from pandas_datareader) (4.2.5)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests>=2.3.0->pandas_datareader) (2.7)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests>=2.3.0->pandas_datareader) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests>=2.3.0->pandas_datareader) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests>=2.3.0->pandas_datareader) (2018.11.29)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/site-packages (from pandas>=0.19.2->pandas_datareader) (2018.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/site-packages (from pandas>=0.19.2->pandas_datareader) (2.7.3)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/site-packages (from pandas>=0.19.2->pandas_datareader) (1.15.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas>=0.19.2->pandas_datareader) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas_datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data as web\n",
    "import datetime\n",
    "\n",
    "def get_stock_return(stock_name):\n",
    "    start=datetime.datetime(2007, 1, 1)\n",
    "    end=datetime.datetime.today()\n",
    "    print(start,end)\n",
    "    \n",
    "    #Get Stock data from Yahoo Finance\n",
    "    df = web.DataReader('stock_name', 'yahoo', start, end)\n",
    "    df.describe() #Get summary statistics\n",
    "    \n",
    "    #Calculate percent changes\n",
    "    ma1 = df['Close'].pct_change() \n",
    "    \n",
    "    %matplotlib inline\n",
    "    return ma1.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data as web\n",
    "from datetime import datetime\n",
    "import math\n",
    "def stock_analysis(start, end, stock_name):\n",
    "    print(start,end)\n",
    "    stock_df = web.DataReader(stock_name, 'yahoo', start, end)\n",
    "    stock_df['logReturn']=pd.Series([math.log(i) for i in stock_df['Close']/stock_df['Open']],\n",
    "                                 index=stock_df.index)\n",
    "    stock_df.describe() #Get summary statistics\n",
    "    return stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=datetime(2000, 1, 1)\n",
    "end=datetime.today()\n",
    "stock_data=stock_analysis(start, end, stock_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def news_stock_analysis(news_result, stock_data):\n",
    "    headers = ['Name','pos','neg','neu','compound']\n",
    "    news_result=news_result.groupby(['Name'])[headers[1:]].mean()\n",
    "    news_result['Date']=[str(datetime.strptime(i,'%B %d, %Y'))[:10] for i in news_result.index]\n",
    "    news_result.index=range(len(news_result))\n",
    "    \n",
    "    stock_data['Date']=[str(i)[:10] for i in stock_data.index]\n",
    "    stock_data.index=range(len(stock_data))\n",
    "    \n",
    "    compound_data=news_result.merge(stock_data, on='Date', how='left')\n",
    "    return compound_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=news_stock_analysis(news_result, stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.dates\n",
    "def compare_plot(c, data1, data2):\n",
    "    df=pd.DataFrame({'Date':matplotlib.dates.datestr2num(c['Date']),\n",
    "                     'news_compound':np.array(c[data1]),\n",
    "                     'stock_logReturn':np.array(c[data2])})\n",
    "    plt.plot('Date',data1,data=df,marker='', color='olive', linewidth=2)\n",
    "    plt.plot('Date',data2,data=df,marker='', color='blue', linewidth=2, linestyle='dashed')\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
