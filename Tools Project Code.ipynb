{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk import sent_tokenize\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as web\n",
    "import datetime\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys  \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    '''\n",
    "    Authors and users initiate global context.\n",
    "    '''\n",
    "    def __init__(self, stock_code = None, universe_condition = 'DJIA'):\n",
    "        '''\n",
    "        Users choose the stock they are going to predict as well as \n",
    "        the stock universe the prediction based on.\n",
    "        \n",
    "        Params:\n",
    "            stock_code:           stock code, the stock to predict\n",
    "                                        'AAPL' - apple; 'GOOG' - google, etc.\n",
    "            universe_conditon:  stock index, the training of estimator is \n",
    "                                        based on the Index constituent stocks.\n",
    "                                        'DJIA' - Dow Jones Industrial Average, \n",
    "                                        'S&P500' - Standard and Poor 500 Index,\n",
    "                                        'nasdaq100' -  NASDAQ-100.\n",
    "        '''\n",
    "        self.stock_code = stock_code\n",
    "        self.universe_condition = universe_condition\n",
    "        assert universe_condition in ['DJIA', 'S&P500', 'nasdaq100']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class news(Context):\n",
    "    def __init__(self, stock_code, universe_condition):\n",
    "        Context.__init__(self, stock_code, universe_condition)\n",
    "        \n",
    "    def _scrolling_down_page(self, stock_name):\n",
    "        '''\n",
    "        Scroll Down Page to Get More News, use chromedriver to open the web.\n",
    "        '''\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')\n",
    "        url = \"https://finance.yahoo.com/quote/\"+ str(stock_name) +\"/?p=\" + str(stock_name)\n",
    "        driver = webdriver.Chrome(executable_path = '/Users/zhaonian/downloads/chromedriver' ,chrome_options=chrome_options)\n",
    "        driver.get(url)\n",
    "        time.sleep(3) \n",
    "\n",
    "        #Selenium automates browsers, scrolling down the page until contents are fully updated\n",
    "        from selenium.webdriver import ActionChains\n",
    "        for i in range(10): \n",
    "            ActionChains(driver).key_down(Keys.DOWN).perform() \n",
    "            print(f'have finished {i} times')\n",
    "            time.sleep(1)\n",
    "\n",
    "        #Acquire page source code\n",
    "        html_ = driver.page_source.encode('utf-8')\n",
    "        results_page = BeautifulSoup(html_,'lxml')\n",
    "        return results_page\n",
    "        \n",
    "# then we combine all these into a def function\n",
    "# stock_name should be 'AAPL' & 'GOOG' cannot be 'apple' or 'google'\n",
    "    def _get_article_link(self, results_page):\n",
    "        '''\n",
    "        Get links of article about the stock from Yahoo Finance.\n",
    "        '''\n",
    "        all_links = []\n",
    "        #url = \"https://finance.yahoo.com/quote/\"+ str(self.stock_name) +\"/?p=\" + str(self.stock_name)\n",
    "        #send the request and get the response\n",
    "        #response = requests.get(url)\n",
    "        #if not response.status_code == 200:\n",
    "        #    return None\n",
    "        try:\n",
    "            # we find the href contains in the tag_h3\n",
    "            all_h3_tags = results_page.find_all('h3', {'class': \"Mb(5px)\"})\n",
    "            article_link = []\n",
    "            # try to find the href in the tag_h3\n",
    "            for tag in all_h3_tags:\n",
    "                try:\n",
    "                    article_link.append(tag.find('a').get('href'))\n",
    "                except:\n",
    "                    return None \n",
    "            for link in article_link:\n",
    "                if 'http' in link:\n",
    "                    all_links.append(link)\n",
    "                else:\n",
    "                    home_url = 'https://finance.yahoo.com'\n",
    "                    url = home_url + link\n",
    "                    all_links.append(url)\n",
    "            return all_links\n",
    "        except:\n",
    "            return None \n",
    "        \n",
    "    def _get_article_content(self, all_links):\n",
    "        '''\n",
    "        Combine the time and article into a list and construct a list.\n",
    "        '''\n",
    "        article_content = []\n",
    "        try:\n",
    "            for link in all_links:\n",
    "                contents = []\n",
    "                response = requests.get(link)\n",
    "                if not response.status_code == 200:\n",
    "                    return None\n",
    "                result_page = BeautifulSoup(response.content,'lxml')\n",
    "                time_tag = result_page.find('time')\n",
    "                time = time_tag.get_text()\n",
    "                contents.append(time)\n",
    "                try:\n",
    "                    content_tag = result_page.find_all('p',{'class':'canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm'})\n",
    "                    for content in content_tag:\n",
    "                        article = content.get_text()\n",
    "                        contents.append(article)\n",
    "                except:\n",
    "                    return None\n",
    "                article_content.append(contents)  \n",
    "                return article_content\n",
    "        except:\n",
    "            return article_content\n",
    "        \n",
    "    def _artical_format(self, artical_content):\n",
    "        '''\n",
    "        change artical content format for text mining\n",
    "        '''\n",
    "        artical_texts = []\n",
    "        for i in artical_content:\n",
    "            if len(i)>1:\n",
    "                artical_texts.append([i[0]]+[''.join(i[1:])])\n",
    "        return artical_texts\n",
    "\n",
    "    def web_crawling(self, stock_name):\n",
    "        results_page = self._scrolling_down_page(stock_name)\n",
    "        all_links = self._get_article_link(results_page)\n",
    "        article_content = self._get_article_content(all_links)\n",
    "        artical_texts = self._artical_format(article_content)\n",
    "        print('Web crawling for', stock_name, 'done!')\n",
    "        return artical_texts\n",
    "\n",
    "    def vader_comparison(self, artical_texts):\n",
    "        headers = ['Name','pos','neg','neu','compound']\n",
    "        sentiment_result=pd.DataFrame(columns=headers)\n",
    "        #print(\"Name\\t\",'  pos\\t','neg\\t','neu\\t','compound')\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        for i in range(len(artical_texts)):\n",
    "            name = artical_texts[i][0]\n",
    "            sentences = sent_tokenize(artical_texts[i][1])\n",
    "            pos=compound=neu=neg=0\n",
    "            for sentence in sentences:\n",
    "                vs = analyzer.polarity_scores(sentence)\n",
    "                pos+=vs['pos']/(len(sentences))\n",
    "                neu+=vs['neu']/(len(sentences))\n",
    "                neg+=vs['neg']/(len(sentences))\n",
    "                compound+=vs['compound']/(len(sentences))\n",
    "            sentiment_result=sentiment_result.append(pd.DataFrame([[name,pos,neg,neu,compound]],columns=headers))\n",
    "            #print('%-10s'%name,'%1.2f\\t'%pos,'%1.2f\\t'%neg,'%1.2f\\t'%neu,'%1.2f\\t'%compound)\n",
    "        sentiment_result=sentiment_result.sort_values(by='Name')\n",
    "        sentiment_result.index=range(len(sentiment_result))\n",
    "        return sentiment_result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-b43ab4138c60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniverse_condition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniverse_condition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_djia_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Context' is not defined"
     ]
    }
   ],
   "source": [
    "class stock(Context):\n",
    "    def __init__(self, stock_code, universe_condition):\n",
    "        Context.__init__(self, stock_code, universe_condition)\n",
    "\n",
    "    def _get_djia_return(self):\n",
    "        djia = requests.get('https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average')\n",
    "        soup = BeautifulSoup(djia.text, 'lxml')\n",
    "        table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "        self.codes = []\n",
    "        for row in table.findAll('tr')[1:]:\n",
    "            code = row.findAll('td')[2].text\n",
    "            self.codes.append(code[:-1])\n",
    "    \n",
    "    def _get_sp500_codes(self):\n",
    "        sp500 = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "        soup = BeautifulSoup(sp500.text, 'lxml')\n",
    "        table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "        self.codes = []\n",
    "        for row in table.findAll('tr')[1:]:\n",
    "            code = row.findAll('td')[0].text\n",
    "            self.codes.append(code)\n",
    "        \n",
    "    def _get_nasdaq100_codes(self):\n",
    "        nasdaq = requests.get('https://en.wikipedia.org/wiki/NASDAQ-100')\n",
    "        soup = BeautifulSoup(nasdaq.text, 'lxml')\n",
    "        table = soup.find('div', class_='div-col columns column-width')\n",
    "        self.codes = []\n",
    "        pattern = r'\\(([A-Z]+)\\)'\n",
    "        for row in table.findAll('li'):\n",
    "            a = re.findall(pattern, str(row.text))\n",
    "            self.codes.append(a[0][:-1])\n",
    "    \n",
    "    def get_stock_return(self):\n",
    "        start=datetime.datetime.today() - datetime.timedelta(4)\n",
    "        end=datetime.datetime.today()\n",
    "        self.ret_all = []\n",
    "        if self.universe_condition == 'DJIA':\n",
    "            self._get_djia_return()\n",
    "        elif self.universe_condition == 'S&P500':\n",
    "            self._get_sp500_codes()\n",
    "        elif self.universe_condition == 'nasdaq100':\n",
    "            self._get_nasdaq100_codes()\n",
    "        for i in self.codes:\n",
    "            df = web.DataReader(i, 'yahoo', start, end)\n",
    "            ret = df['Close'].pct_change()[-1]\n",
    "            self.ret_all.append(ret)\n",
    "        #%matplotlib inline\n",
    "        #return self.ret_all#, ma1.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-45e3565070fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mnews_stock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniverse_condition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniverse_condition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'news' is not defined"
     ]
    }
   ],
   "source": [
    "class news_stock(news, stock):\n",
    "    def __init__(self, stock_code, universe_condition):\n",
    "        Context.__init__(self, stock_code, universe_condition)\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        sentiment_com_tr = []\n",
    "        sentiment_pos_tr = []\n",
    "        sentiment_neg_tr = []\n",
    "        sentiment_com_te = []\n",
    "        sentiment_pos_te = []\n",
    "        sentiment_neg_te = []\n",
    "        error_code = []\n",
    "        for i in self.codes:\n",
    "            #try:\n",
    "            artical_texts = news.web_crawling(self, i)\n",
    "            a = news.vader_comparison(self, artical_texts)\n",
    "            a['Name'] = pd.to_datetime(a['Name'])\n",
    "            a = a.sort_values(['Name'],ascending = 0)\n",
    "            a = a.fillna(0)\n",
    "            a_train = a[-round(len(a)*0.7):]\n",
    "            a_test = a[:round(len(a)*0.3)]\n",
    "            sentiment_com_tr.append(a_train.mean()['compound'])\n",
    "            sentiment_pos_tr.append(a_train.mean()['pos'])\n",
    "            sentiment_neg_tr.append(a_train.mean()['neg'])\n",
    "            sentiment_com_te.append(a_test.mean()['compound'])\n",
    "            sentiment_pos_te.append(a_test.mean()['pos'])\n",
    "            sentiment_neg_te.append(a_test.mean()['neg'])\n",
    "            #except:\n",
    "             #   error_code.append(i)\n",
    "        self.sentiment = [sentiment_com_tr,sentiment_pos_tr,sentiment_neg_tr,sentiment_com_te,sentiment_pos_te,sentiment_neg_te,error_code]\n",
    "    \n",
    "    def regress(self):\n",
    "        X = np.column_stack((self.sentiment[0], self.sentiment[1], self.sentiment[2]))\n",
    "        X = sm.add_constant(X)\n",
    "        X = np.nan_to_num(X)\n",
    "        y = self.ret_all\n",
    "        model = sm.OLS(y,X)\n",
    "        self.OLS_results = model.fit()\n",
    "        #print(results.params)\n",
    "        \n",
    "    def prediction(self):\n",
    "        if isinstance(self.stock_code, str):\n",
    "            stock_code = [self.stock_code] \n",
    "        self.predict_result_all = []\n",
    "        for i in range(len(stock_code)):\n",
    "            predict_return = np.dot([1, self.sentiment[3][i],self.sentiment[4][i],self.sentiment[5][i]],self.OLS_results.params)\n",
    "            print('Predicted return for stock ', self.stock_code, ' is ', predict_return)\n",
    "            self.predict_result_all.append(predict_return)\n",
    "        return self.predict_result\n",
    "    \n",
    "    def visulization(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = news_stock('AAPL','DJIA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.get_stock_return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.get_sentiment()a.regress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.regress()\n",
    "a.prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Web Crawling for Stock Related News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys  \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import time\n",
    "\n",
    "def scrolling_down_page(stock_name):\n",
    "    #Using chromedriver to open the web\n",
    "    url = \"https://finance.yahoo.com/quote/\"+ str(stock_name) +\"/?p=\" + str(stock_name)\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3) \n",
    "    \n",
    "    #Selenium automates browsers, scrolling down the page until contents are fully updated\n",
    "    from selenium.webdriver import ActionChains\n",
    "    for i in range(1000): \n",
    "        ActionChains(driver).key_down(Keys.DOWN).perform() \n",
    "        print(f'have finished {i} times')\n",
    "        time.sleep(1)\n",
    "    \n",
    "    #Acquire page source code\n",
    "    html_ = driver.page_source.encode('utf-8')\n",
    "    results_page = BeautifulSoup(html_,'lxml')\n",
    "    \n",
    "    return results_page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_link(results_page):\n",
    "    all_links = []\n",
    "    #url = \"https://finance.yahoo.com/quote/\"+ str(stock_name) +\"/?p=\" + str(stock_name)\n",
    "    #send the request and get the response\n",
    "    #response = requests.get(url)\n",
    "    #if not response.status_code == 200:\n",
    "    #    return None\n",
    "    try:\n",
    "        #results_page = BeautifulSoup(response.content,'lxml')\n",
    "        # we find the href contains in the tag_h3\n",
    "        all_h3_tags = results_page.find_all('h3', {'class': \"Mb(5px)\"})\n",
    "        article_link = []\n",
    "        # try to find the href in the tag_h3\n",
    "        for tag in all_h3_tags:\n",
    "            try:\n",
    "                article_link.append(tag.find('a').get('href'))\n",
    "            except:\n",
    "                return None\n",
    "        for link in article_link:\n",
    "            if 'http' in link:\n",
    "                #all_links.append(url)\n",
    "                all_links.append(link)\n",
    "            else:\n",
    "                home_url = 'https://finance.yahoo.com'\n",
    "                url = home_url + link\n",
    "                all_links.append(url)\n",
    "        return all_links\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_content(all_links):\n",
    "    article_content = []\n",
    "    try:\n",
    "        for link in all_links:\n",
    "            if link[:26] != 'https://finance.yahoo.com/':\n",
    "                continue\n",
    "            contents = []\n",
    "            response = requests.get(link)\n",
    "            if not response.status_code == 200:\n",
    "                return None\n",
    "            result_page = BeautifulSoup(response.content,'lxml')\n",
    "            time_tag = result_page.find('time')\n",
    "            time = time_tag.get_text()\n",
    "            contents.append(time)\n",
    "            try:\n",
    "                content_tag = result_page.find_all('p',{'class':'canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm'})\n",
    "                for content in content_tag:\n",
    "                    article = content.get_text()\n",
    "                    contents.append(article)\n",
    "            except:\n",
    "                return None\n",
    "            article_content.append(contents)  \n",
    "        return article_content\n",
    "    except:\n",
    "        return article_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = 'AAPL'\n",
    "results_page = scrolling_down_page(stock_name)\n",
    "all_links=get_article_link(results_page)\n",
    "artical_content=get_article_content(all_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change artical content format for text mining\n",
    "def artical_format(artical_content):\n",
    "    artical_texts = []\n",
    "    for i in artical_content:\n",
    "        if len(i)>1:\n",
    "            artical_texts.append([i[0]]+[''.join(i[1:])])\n",
    "    return artical_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return text analysis results\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk import sent_tokenize\n",
    "import pandas as pd\n",
    "def vader_comparison(texts):\n",
    "    headers = ['Name','pos','neg','neu','compound']\n",
    "    result=pd.DataFrame(columns=headers)\n",
    "    print(\"Name\\t\",'  pos\\t','neg\\t','neu\\t','compound')\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for i in range(len(texts)):\n",
    "        name = texts[i][0]\n",
    "        sentences = sent_tokenize(texts[i][1])\n",
    "        pos=compound=neu=neg=0\n",
    "        for sentence in sentences:\n",
    "            vs = analyzer.polarity_scores(sentence)\n",
    "            pos+=vs['pos']/(len(sentences))\n",
    "            neu+=vs['neu']/(len(sentences))\n",
    "            neg+=vs['neg']/(len(sentences))\n",
    "            compound+=vs['compound']/(len(sentences))\n",
    "        result=result.append(pd.DataFrame([[name,pos,neg,neu,compound]],columns=headers))\n",
    "        print('%-10s'%name,'%1.2f\\t'%pos,'%1.2f\\t'%neg,'%1.2f\\t'%neu,'%1.2f\\t'%compound)\n",
    "    result=result.sort_values(by='Name')\n",
    "    result.index=range(len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artical_texts=artical_format(artical_content)\n",
    "news_result=vader_comparison(artical_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Acquire Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_djia_return():\n",
    "    djia = requests.get('https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average')\n",
    "    soup = BeautifulSoup(djia.text, 'lxml')\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    codes = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        code = row.findAll('td')[2].text\n",
    "        codes.append(code[:-1])\n",
    "    return codes\n",
    "    \n",
    "def _get_sp500_codes():\n",
    "    sp500 = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = BeautifulSoup(sp500.text, 'lxml')\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    codes = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        code = row.findAll('td')[0].text\n",
    "        codes.append(code)\n",
    "    return codes\n",
    "        \n",
    "def _get_nasdaq100_codes():\n",
    "    nasdaq = requests.get('https://en.wikipedia.org/wiki/NASDAQ-100')\n",
    "    soup = BeautifulSoup(nasdaq.text, 'lxml')\n",
    "    table = soup.find('div', class_='div-col columns column-width')\n",
    "    codes = []\n",
    "    pattern = r'\\(([A-Z]+)\\)'\n",
    "    for row in table.findAll('li'):\n",
    "        a = re.findall(pattern, str(row.text))\n",
    "        codes.append(a[0][:-1])\n",
    "    return codes\n",
    "    \n",
    "def get_stock_return(universe_condition):\n",
    "    start=datetime.datetime.today() - datetime.timedelta(4)\n",
    "    end=datetime.datetime.today()\n",
    "    self.ret_all = []\n",
    "    if universe_condition == 'DJIA':\n",
    "        codes = _get_djia_return()\n",
    "    elif universe_condition == 'S&P500':\n",
    "        codes = _get_sp500_codes()\n",
    "    elif universe_condition == 'nasdaq100':\n",
    "        codes = _get_nasdaq100_codes()\n",
    "    for i in codes:\n",
    "        df = web.DataReader(i, 'yahoo', start, end)\n",
    "        ret = df['Close'].pct_change()[-1]\n",
    "        ret_all.append(ret)\n",
    "    return ret_all, codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_return, stock_codes = get_stock_return('DJIA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Virtualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data as web\n",
    "from datetime import datetime\n",
    "import math\n",
    "def stock_analysis(start, end, stock_name):\n",
    "    print(start,end)\n",
    "    stock_df = web.DataReader(stock_name, 'yahoo', start, end)\n",
    "    stock_df['logReturn']=pd.Series([math.log(i) for i in stock_df['Close']/stock_df['Open']],\n",
    "                                 index=stock_df.index)\n",
    "    stock_df.describe() #Get summary statistics\n",
    "    return stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01 00:00:00 2018-12-02 21:29:58.182401\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>logReturn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>4.017857</td>\n",
       "      <td>3.631696</td>\n",
       "      <td>3.745536</td>\n",
       "      <td>3.997768</td>\n",
       "      <td>133949200.0</td>\n",
       "      <td>2.677157</td>\n",
       "      <td>0.065172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>3.950893</td>\n",
       "      <td>3.613839</td>\n",
       "      <td>3.866071</td>\n",
       "      <td>3.660714</td>\n",
       "      <td>128094400.0</td>\n",
       "      <td>2.451444</td>\n",
       "      <td>-0.054581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>3.948661</td>\n",
       "      <td>3.678571</td>\n",
       "      <td>3.705357</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>194580400.0</td>\n",
       "      <td>2.487319</td>\n",
       "      <td>0.002407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>3.821429</td>\n",
       "      <td>3.392857</td>\n",
       "      <td>3.790179</td>\n",
       "      <td>3.392857</td>\n",
       "      <td>191993200.0</td>\n",
       "      <td>2.272070</td>\n",
       "      <td>-0.110741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>3.607143</td>\n",
       "      <td>3.410714</td>\n",
       "      <td>3.446429</td>\n",
       "      <td>3.553571</td>\n",
       "      <td>115183600.0</td>\n",
       "      <td>2.379695</td>\n",
       "      <td>0.030615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-10</th>\n",
       "      <td>3.651786</td>\n",
       "      <td>3.383929</td>\n",
       "      <td>3.642857</td>\n",
       "      <td>3.491071</td>\n",
       "      <td>126266000.0</td>\n",
       "      <td>2.337840</td>\n",
       "      <td>-0.042560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-11</th>\n",
       "      <td>3.549107</td>\n",
       "      <td>3.232143</td>\n",
       "      <td>3.426339</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>110387200.0</td>\n",
       "      <td>2.218258</td>\n",
       "      <td>-0.033789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-12</th>\n",
       "      <td>3.410714</td>\n",
       "      <td>3.089286</td>\n",
       "      <td>3.392857</td>\n",
       "      <td>3.113839</td>\n",
       "      <td>244017200.0</td>\n",
       "      <td>2.085222</td>\n",
       "      <td>-0.085816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-13</th>\n",
       "      <td>3.526786</td>\n",
       "      <td>3.303571</td>\n",
       "      <td>3.374439</td>\n",
       "      <td>3.455357</td>\n",
       "      <td>258171200.0</td>\n",
       "      <td>2.313924</td>\n",
       "      <td>0.023697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-14</th>\n",
       "      <td>3.651786</td>\n",
       "      <td>3.549107</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>3.587054</td>\n",
       "      <td>97594000.0</td>\n",
       "      <td>2.402116</td>\n",
       "      <td>0.004365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-18</th>\n",
       "      <td>3.785714</td>\n",
       "      <td>3.587054</td>\n",
       "      <td>3.607143</td>\n",
       "      <td>3.712054</td>\n",
       "      <td>114794400.0</td>\n",
       "      <td>2.485824</td>\n",
       "      <td>0.028669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-19</th>\n",
       "      <td>3.883929</td>\n",
       "      <td>3.691964</td>\n",
       "      <td>3.772321</td>\n",
       "      <td>3.805804</td>\n",
       "      <td>149410800.0</td>\n",
       "      <td>2.548605</td>\n",
       "      <td>0.008837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-20</th>\n",
       "      <td>4.339286</td>\n",
       "      <td>4.053571</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>4.053571</td>\n",
       "      <td>457783200.0</td>\n",
       "      <td>2.714526</td>\n",
       "      <td>-0.017468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-21</th>\n",
       "      <td>4.080357</td>\n",
       "      <td>3.935268</td>\n",
       "      <td>4.080357</td>\n",
       "      <td>3.975446</td>\n",
       "      <td>123981200.0</td>\n",
       "      <td>2.662209</td>\n",
       "      <td>-0.026047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-24</th>\n",
       "      <td>4.026786</td>\n",
       "      <td>3.754464</td>\n",
       "      <td>3.872768</td>\n",
       "      <td>3.794643</td>\n",
       "      <td>110219200.0</td>\n",
       "      <td>2.541132</td>\n",
       "      <td>-0.020379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-25</th>\n",
       "      <td>4.040179</td>\n",
       "      <td>3.656250</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>4.008929</td>\n",
       "      <td>124286400.0</td>\n",
       "      <td>2.684630</td>\n",
       "      <td>0.066768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-26</th>\n",
       "      <td>4.078125</td>\n",
       "      <td>3.919643</td>\n",
       "      <td>3.928571</td>\n",
       "      <td>3.935268</td>\n",
       "      <td>91789600.0</td>\n",
       "      <td>2.635303</td>\n",
       "      <td>0.001703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-27</th>\n",
       "      <td>4.035714</td>\n",
       "      <td>3.821429</td>\n",
       "      <td>3.886161</td>\n",
       "      <td>3.928571</td>\n",
       "      <td>85036000.0</td>\n",
       "      <td>2.630817</td>\n",
       "      <td>0.010854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>3.959821</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>3.863839</td>\n",
       "      <td>3.629464</td>\n",
       "      <td>105837200.0</td>\n",
       "      <td>2.430518</td>\n",
       "      <td>-0.062576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-31</th>\n",
       "      <td>3.709821</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>3.607143</td>\n",
       "      <td>3.705357</td>\n",
       "      <td>175420000.0</td>\n",
       "      <td>2.481340</td>\n",
       "      <td>0.026864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-01</th>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>3.580357</td>\n",
       "      <td>79508800.0</td>\n",
       "      <td>2.397632</td>\n",
       "      <td>-0.036724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-02</th>\n",
       "      <td>3.647321</td>\n",
       "      <td>3.464286</td>\n",
       "      <td>3.598214</td>\n",
       "      <td>3.529018</td>\n",
       "      <td>116048800.0</td>\n",
       "      <td>2.363252</td>\n",
       "      <td>-0.019418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-03</th>\n",
       "      <td>3.723214</td>\n",
       "      <td>3.580357</td>\n",
       "      <td>3.582589</td>\n",
       "      <td>3.689732</td>\n",
       "      <td>118798400.0</td>\n",
       "      <td>2.470876</td>\n",
       "      <td>0.029468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-04</th>\n",
       "      <td>3.928571</td>\n",
       "      <td>3.700893</td>\n",
       "      <td>3.712054</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>106330000.0</td>\n",
       "      <td>2.582985</td>\n",
       "      <td>0.038341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-07</th>\n",
       "      <td>4.080357</td>\n",
       "      <td>3.783482</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>4.073661</td>\n",
       "      <td>110266800.0</td>\n",
       "      <td>2.727979</td>\n",
       "      <td>0.054615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-08</th>\n",
       "      <td>4.147321</td>\n",
       "      <td>3.973214</td>\n",
       "      <td>4.071429</td>\n",
       "      <td>4.102679</td>\n",
       "      <td>102160800.0</td>\n",
       "      <td>2.747411</td>\n",
       "      <td>0.007646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-09</th>\n",
       "      <td>4.183036</td>\n",
       "      <td>4.015625</td>\n",
       "      <td>4.075893</td>\n",
       "      <td>4.022321</td>\n",
       "      <td>74841200.0</td>\n",
       "      <td>2.693599</td>\n",
       "      <td>-0.013231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-10</th>\n",
       "      <td>4.066964</td>\n",
       "      <td>3.928571</td>\n",
       "      <td>4.031250</td>\n",
       "      <td>4.053571</td>\n",
       "      <td>75745600.0</td>\n",
       "      <td>2.714526</td>\n",
       "      <td>0.005522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-11</th>\n",
       "      <td>4.075893</td>\n",
       "      <td>3.866071</td>\n",
       "      <td>4.058036</td>\n",
       "      <td>3.883929</td>\n",
       "      <td>53062800.0</td>\n",
       "      <td>2.600923</td>\n",
       "      <td>-0.043852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-14</th>\n",
       "      <td>4.138393</td>\n",
       "      <td>3.879464</td>\n",
       "      <td>3.904018</td>\n",
       "      <td>4.136161</td>\n",
       "      <td>91884800.0</td>\n",
       "      <td>2.769833</td>\n",
       "      <td>0.057762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-19</th>\n",
       "      <td>221.259995</td>\n",
       "      <td>217.429993</td>\n",
       "      <td>218.059998</td>\n",
       "      <td>219.309998</td>\n",
       "      <td>33078700.0</td>\n",
       "      <td>218.547455</td>\n",
       "      <td>0.005716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-22</th>\n",
       "      <td>223.360001</td>\n",
       "      <td>218.940002</td>\n",
       "      <td>219.789993</td>\n",
       "      <td>220.649994</td>\n",
       "      <td>28792100.0</td>\n",
       "      <td>219.882782</td>\n",
       "      <td>0.003905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-23</th>\n",
       "      <td>223.250000</td>\n",
       "      <td>214.699997</td>\n",
       "      <td>215.830002</td>\n",
       "      <td>222.729996</td>\n",
       "      <td>38767800.0</td>\n",
       "      <td>221.955551</td>\n",
       "      <td>0.031469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-24</th>\n",
       "      <td>224.229996</td>\n",
       "      <td>214.539993</td>\n",
       "      <td>222.600006</td>\n",
       "      <td>215.089996</td>\n",
       "      <td>40925500.0</td>\n",
       "      <td>214.342117</td>\n",
       "      <td>-0.034320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-25</th>\n",
       "      <td>221.380005</td>\n",
       "      <td>216.750000</td>\n",
       "      <td>217.710007</td>\n",
       "      <td>219.800003</td>\n",
       "      <td>29855800.0</td>\n",
       "      <td>219.035751</td>\n",
       "      <td>0.009554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-26</th>\n",
       "      <td>220.190002</td>\n",
       "      <td>212.669998</td>\n",
       "      <td>215.899994</td>\n",
       "      <td>216.300003</td>\n",
       "      <td>47258400.0</td>\n",
       "      <td>215.547913</td>\n",
       "      <td>0.001851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-29</th>\n",
       "      <td>219.690002</td>\n",
       "      <td>206.089996</td>\n",
       "      <td>219.190002</td>\n",
       "      <td>212.240005</td>\n",
       "      <td>45935500.0</td>\n",
       "      <td>211.502045</td>\n",
       "      <td>-0.032221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-30</th>\n",
       "      <td>215.179993</td>\n",
       "      <td>209.270004</td>\n",
       "      <td>211.149994</td>\n",
       "      <td>213.300003</td>\n",
       "      <td>36660000.0</td>\n",
       "      <td>212.558350</td>\n",
       "      <td>0.010131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>220.449997</td>\n",
       "      <td>216.619995</td>\n",
       "      <td>216.880005</td>\n",
       "      <td>218.860001</td>\n",
       "      <td>38358900.0</td>\n",
       "      <td>218.099014</td>\n",
       "      <td>0.009088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-01</th>\n",
       "      <td>222.360001</td>\n",
       "      <td>216.809998</td>\n",
       "      <td>219.050003</td>\n",
       "      <td>222.220001</td>\n",
       "      <td>58323200.0</td>\n",
       "      <td>221.447327</td>\n",
       "      <td>0.014368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-02</th>\n",
       "      <td>213.649994</td>\n",
       "      <td>205.429993</td>\n",
       "      <td>209.550003</td>\n",
       "      <td>207.479996</td>\n",
       "      <td>91328700.0</td>\n",
       "      <td>206.758575</td>\n",
       "      <td>-0.009927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-05</th>\n",
       "      <td>204.389999</td>\n",
       "      <td>198.169998</td>\n",
       "      <td>204.300003</td>\n",
       "      <td>201.589996</td>\n",
       "      <td>66163700.0</td>\n",
       "      <td>200.889053</td>\n",
       "      <td>-0.013354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-06</th>\n",
       "      <td>204.720001</td>\n",
       "      <td>201.690002</td>\n",
       "      <td>201.919998</td>\n",
       "      <td>203.770004</td>\n",
       "      <td>31882900.0</td>\n",
       "      <td>203.061493</td>\n",
       "      <td>0.009120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-07</th>\n",
       "      <td>210.059998</td>\n",
       "      <td>204.130005</td>\n",
       "      <td>205.970001</td>\n",
       "      <td>209.949997</td>\n",
       "      <td>33424400.0</td>\n",
       "      <td>209.219986</td>\n",
       "      <td>0.019139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-08</th>\n",
       "      <td>210.119995</td>\n",
       "      <td>206.750000</td>\n",
       "      <td>209.979996</td>\n",
       "      <td>208.490005</td>\n",
       "      <td>25362600.0</td>\n",
       "      <td>208.490005</td>\n",
       "      <td>-0.007121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-09</th>\n",
       "      <td>206.009995</td>\n",
       "      <td>202.250000</td>\n",
       "      <td>205.550003</td>\n",
       "      <td>204.470001</td>\n",
       "      <td>34365800.0</td>\n",
       "      <td>204.470001</td>\n",
       "      <td>-0.005268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-12</th>\n",
       "      <td>199.850006</td>\n",
       "      <td>193.789993</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>194.169998</td>\n",
       "      <td>51135500.0</td>\n",
       "      <td>194.169998</td>\n",
       "      <td>-0.024571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-13</th>\n",
       "      <td>197.179993</td>\n",
       "      <td>191.449997</td>\n",
       "      <td>191.630005</td>\n",
       "      <td>192.229996</td>\n",
       "      <td>46882900.0</td>\n",
       "      <td>192.229996</td>\n",
       "      <td>0.003126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-14</th>\n",
       "      <td>194.479996</td>\n",
       "      <td>185.929993</td>\n",
       "      <td>193.899994</td>\n",
       "      <td>186.800003</td>\n",
       "      <td>60801000.0</td>\n",
       "      <td>186.800003</td>\n",
       "      <td>-0.037304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-15</th>\n",
       "      <td>191.970001</td>\n",
       "      <td>186.899994</td>\n",
       "      <td>188.389999</td>\n",
       "      <td>191.410004</td>\n",
       "      <td>46478800.0</td>\n",
       "      <td>191.410004</td>\n",
       "      <td>0.015903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-16</th>\n",
       "      <td>194.970001</td>\n",
       "      <td>189.460007</td>\n",
       "      <td>190.500000</td>\n",
       "      <td>193.529999</td>\n",
       "      <td>36928300.0</td>\n",
       "      <td>193.529999</td>\n",
       "      <td>0.015780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-19</th>\n",
       "      <td>190.699997</td>\n",
       "      <td>184.990005</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>185.860001</td>\n",
       "      <td>41925300.0</td>\n",
       "      <td>185.860001</td>\n",
       "      <td>-0.022030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-20</th>\n",
       "      <td>181.470001</td>\n",
       "      <td>175.509995</td>\n",
       "      <td>178.369995</td>\n",
       "      <td>176.979996</td>\n",
       "      <td>67825200.0</td>\n",
       "      <td>176.979996</td>\n",
       "      <td>-0.007823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-21</th>\n",
       "      <td>180.270004</td>\n",
       "      <td>176.550003</td>\n",
       "      <td>179.729996</td>\n",
       "      <td>176.779999</td>\n",
       "      <td>31124200.0</td>\n",
       "      <td>176.779999</td>\n",
       "      <td>-0.016550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-23</th>\n",
       "      <td>176.600006</td>\n",
       "      <td>172.100006</td>\n",
       "      <td>174.940002</td>\n",
       "      <td>172.289993</td>\n",
       "      <td>23624000.0</td>\n",
       "      <td>172.289993</td>\n",
       "      <td>-0.015264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-26</th>\n",
       "      <td>174.949997</td>\n",
       "      <td>170.259995</td>\n",
       "      <td>174.240005</td>\n",
       "      <td>174.619995</td>\n",
       "      <td>44738600.0</td>\n",
       "      <td>174.619995</td>\n",
       "      <td>0.002178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-27</th>\n",
       "      <td>174.770004</td>\n",
       "      <td>170.880005</td>\n",
       "      <td>171.509995</td>\n",
       "      <td>174.240005</td>\n",
       "      <td>41387400.0</td>\n",
       "      <td>174.240005</td>\n",
       "      <td>0.015792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-28</th>\n",
       "      <td>181.289993</td>\n",
       "      <td>174.929993</td>\n",
       "      <td>176.729996</td>\n",
       "      <td>180.940002</td>\n",
       "      <td>46062500.0</td>\n",
       "      <td>180.940002</td>\n",
       "      <td>0.023542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-29</th>\n",
       "      <td>182.800003</td>\n",
       "      <td>177.699997</td>\n",
       "      <td>182.660004</td>\n",
       "      <td>179.550003</td>\n",
       "      <td>41770000.0</td>\n",
       "      <td>179.550003</td>\n",
       "      <td>-0.017173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>180.330002</td>\n",
       "      <td>177.029999</td>\n",
       "      <td>180.289993</td>\n",
       "      <td>178.580002</td>\n",
       "      <td>39483800.0</td>\n",
       "      <td>178.580002</td>\n",
       "      <td>-0.009530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4760 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  High         Low        Open       Close       Volume  \\\n",
       "Date                                                                      \n",
       "2000-01-03    4.017857    3.631696    3.745536    3.997768  133949200.0   \n",
       "2000-01-04    3.950893    3.613839    3.866071    3.660714  128094400.0   \n",
       "2000-01-05    3.948661    3.678571    3.705357    3.714286  194580400.0   \n",
       "2000-01-06    3.821429    3.392857    3.790179    3.392857  191993200.0   \n",
       "2000-01-07    3.607143    3.410714    3.446429    3.553571  115183600.0   \n",
       "2000-01-10    3.651786    3.383929    3.642857    3.491071  126266000.0   \n",
       "2000-01-11    3.549107    3.232143    3.426339    3.312500  110387200.0   \n",
       "2000-01-12    3.410714    3.089286    3.392857    3.113839  244017200.0   \n",
       "2000-01-13    3.526786    3.303571    3.374439    3.455357  258171200.0   \n",
       "2000-01-14    3.651786    3.549107    3.571429    3.587054   97594000.0   \n",
       "2000-01-18    3.785714    3.587054    3.607143    3.712054  114794400.0   \n",
       "2000-01-19    3.883929    3.691964    3.772321    3.805804  149410800.0   \n",
       "2000-01-20    4.339286    4.053571    4.125000    4.053571  457783200.0   \n",
       "2000-01-21    4.080357    3.935268    4.080357    3.975446  123981200.0   \n",
       "2000-01-24    4.026786    3.754464    3.872768    3.794643  110219200.0   \n",
       "2000-01-25    4.040179    3.656250    3.750000    4.008929  124286400.0   \n",
       "2000-01-26    4.078125    3.919643    3.928571    3.935268   91789600.0   \n",
       "2000-01-27    4.035714    3.821429    3.886161    3.928571   85036000.0   \n",
       "2000-01-28    3.959821    3.593750    3.863839    3.629464  105837200.0   \n",
       "2000-01-31    3.709821    3.375000    3.607143    3.705357  175420000.0   \n",
       "2000-02-01    3.750000    3.571429    3.714286    3.580357   79508800.0   \n",
       "2000-02-02    3.647321    3.464286    3.598214    3.529018  116048800.0   \n",
       "2000-02-03    3.723214    3.580357    3.582589    3.689732  118798400.0   \n",
       "2000-02-04    3.928571    3.700893    3.712054    3.857143  106330000.0   \n",
       "2000-02-07    4.080357    3.783482    3.857143    4.073661  110266800.0   \n",
       "2000-02-08    4.147321    3.973214    4.071429    4.102679  102160800.0   \n",
       "2000-02-09    4.183036    4.015625    4.075893    4.022321   74841200.0   \n",
       "2000-02-10    4.066964    3.928571    4.031250    4.053571   75745600.0   \n",
       "2000-02-11    4.075893    3.866071    4.058036    3.883929   53062800.0   \n",
       "2000-02-14    4.138393    3.879464    3.904018    4.136161   91884800.0   \n",
       "...                ...         ...         ...         ...          ...   \n",
       "2018-10-19  221.259995  217.429993  218.059998  219.309998   33078700.0   \n",
       "2018-10-22  223.360001  218.940002  219.789993  220.649994   28792100.0   \n",
       "2018-10-23  223.250000  214.699997  215.830002  222.729996   38767800.0   \n",
       "2018-10-24  224.229996  214.539993  222.600006  215.089996   40925500.0   \n",
       "2018-10-25  221.380005  216.750000  217.710007  219.800003   29855800.0   \n",
       "2018-10-26  220.190002  212.669998  215.899994  216.300003   47258400.0   \n",
       "2018-10-29  219.690002  206.089996  219.190002  212.240005   45935500.0   \n",
       "2018-10-30  215.179993  209.270004  211.149994  213.300003   36660000.0   \n",
       "2018-10-31  220.449997  216.619995  216.880005  218.860001   38358900.0   \n",
       "2018-11-01  222.360001  216.809998  219.050003  222.220001   58323200.0   \n",
       "2018-11-02  213.649994  205.429993  209.550003  207.479996   91328700.0   \n",
       "2018-11-05  204.389999  198.169998  204.300003  201.589996   66163700.0   \n",
       "2018-11-06  204.720001  201.690002  201.919998  203.770004   31882900.0   \n",
       "2018-11-07  210.059998  204.130005  205.970001  209.949997   33424400.0   \n",
       "2018-11-08  210.119995  206.750000  209.979996  208.490005   25362600.0   \n",
       "2018-11-09  206.009995  202.250000  205.550003  204.470001   34365800.0   \n",
       "2018-11-12  199.850006  193.789993  199.000000  194.169998   51135500.0   \n",
       "2018-11-13  197.179993  191.449997  191.630005  192.229996   46882900.0   \n",
       "2018-11-14  194.479996  185.929993  193.899994  186.800003   60801000.0   \n",
       "2018-11-15  191.970001  186.899994  188.389999  191.410004   46478800.0   \n",
       "2018-11-16  194.970001  189.460007  190.500000  193.529999   36928300.0   \n",
       "2018-11-19  190.699997  184.990005  190.000000  185.860001   41925300.0   \n",
       "2018-11-20  181.470001  175.509995  178.369995  176.979996   67825200.0   \n",
       "2018-11-21  180.270004  176.550003  179.729996  176.779999   31124200.0   \n",
       "2018-11-23  176.600006  172.100006  174.940002  172.289993   23624000.0   \n",
       "2018-11-26  174.949997  170.259995  174.240005  174.619995   44738600.0   \n",
       "2018-11-27  174.770004  170.880005  171.509995  174.240005   41387400.0   \n",
       "2018-11-28  181.289993  174.929993  176.729996  180.940002   46062500.0   \n",
       "2018-11-29  182.800003  177.699997  182.660004  179.550003   41770000.0   \n",
       "2018-11-30  180.330002  177.029999  180.289993  178.580002   39483800.0   \n",
       "\n",
       "             Adj Close  logReturn  \n",
       "Date                               \n",
       "2000-01-03    2.677157   0.065172  \n",
       "2000-01-04    2.451444  -0.054581  \n",
       "2000-01-05    2.487319   0.002407  \n",
       "2000-01-06    2.272070  -0.110741  \n",
       "2000-01-07    2.379695   0.030615  \n",
       "2000-01-10    2.337840  -0.042560  \n",
       "2000-01-11    2.218258  -0.033789  \n",
       "2000-01-12    2.085222  -0.085816  \n",
       "2000-01-13    2.313924   0.023697  \n",
       "2000-01-14    2.402116   0.004365  \n",
       "2000-01-18    2.485824   0.028669  \n",
       "2000-01-19    2.548605   0.008837  \n",
       "2000-01-20    2.714526  -0.017468  \n",
       "2000-01-21    2.662209  -0.026047  \n",
       "2000-01-24    2.541132  -0.020379  \n",
       "2000-01-25    2.684630   0.066768  \n",
       "2000-01-26    2.635303   0.001703  \n",
       "2000-01-27    2.630817   0.010854  \n",
       "2000-01-28    2.430518  -0.062576  \n",
       "2000-01-31    2.481340   0.026864  \n",
       "2000-02-01    2.397632  -0.036724  \n",
       "2000-02-02    2.363252  -0.019418  \n",
       "2000-02-03    2.470876   0.029468  \n",
       "2000-02-04    2.582985   0.038341  \n",
       "2000-02-07    2.727979   0.054615  \n",
       "2000-02-08    2.747411   0.007646  \n",
       "2000-02-09    2.693599  -0.013231  \n",
       "2000-02-10    2.714526   0.005522  \n",
       "2000-02-11    2.600923  -0.043852  \n",
       "2000-02-14    2.769833   0.057762  \n",
       "...                ...        ...  \n",
       "2018-10-19  218.547455   0.005716  \n",
       "2018-10-22  219.882782   0.003905  \n",
       "2018-10-23  221.955551   0.031469  \n",
       "2018-10-24  214.342117  -0.034320  \n",
       "2018-10-25  219.035751   0.009554  \n",
       "2018-10-26  215.547913   0.001851  \n",
       "2018-10-29  211.502045  -0.032221  \n",
       "2018-10-30  212.558350   0.010131  \n",
       "2018-10-31  218.099014   0.009088  \n",
       "2018-11-01  221.447327   0.014368  \n",
       "2018-11-02  206.758575  -0.009927  \n",
       "2018-11-05  200.889053  -0.013354  \n",
       "2018-11-06  203.061493   0.009120  \n",
       "2018-11-07  209.219986   0.019139  \n",
       "2018-11-08  208.490005  -0.007121  \n",
       "2018-11-09  204.470001  -0.005268  \n",
       "2018-11-12  194.169998  -0.024571  \n",
       "2018-11-13  192.229996   0.003126  \n",
       "2018-11-14  186.800003  -0.037304  \n",
       "2018-11-15  191.410004   0.015903  \n",
       "2018-11-16  193.529999   0.015780  \n",
       "2018-11-19  185.860001  -0.022030  \n",
       "2018-11-20  176.979996  -0.007823  \n",
       "2018-11-21  176.779999  -0.016550  \n",
       "2018-11-23  172.289993  -0.015264  \n",
       "2018-11-26  174.619995   0.002178  \n",
       "2018-11-27  174.240005   0.015792  \n",
       "2018-11-28  180.940002   0.023542  \n",
       "2018-11-29  179.550003  -0.017173  \n",
       "2018-11-30  178.580002  -0.009530  \n",
       "\n",
       "[4760 rows x 7 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_name = 'AAPL'\n",
    "start=datetime(2000, 1, 1)\n",
    "end=datetime.today()\n",
    "stock_data=stock_analysis(start, end, stock_name)\n",
    "stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def news_stock_analysis(news_result, stock_data):\n",
    "    headers = ['Name','pos','neg','neu','compound']\n",
    "    news_result=news_result.groupby(['Name'])[headers[1:]].mean()\n",
    "    news_result['Date']=[str(datetime.strptime(i,'%B %d, %Y'))[:10] for i in news_result.index]\n",
    "    news_result.index=range(len(news_result))\n",
    "    \n",
    "    stock_data['Date']=[str(i)[:10] for i in stock_data.index]\n",
    "    stock_data.index=range(len(stock_data))\n",
    "    \n",
    "    compound_data=news_result.merge(stock_data, on='Date', how='left')\n",
    "    return compound_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataError",
     "evalue": "No numeric types to aggregate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-efcd40a61cf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnews_stock_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-4334cf37f206>\u001b[0m in \u001b[0;36mnews_stock_analysis\u001b[0;34m(news_result, stock_data)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnews_stock_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'neu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'compound'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnews_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnews_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnews_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%B %d, %Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnews_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnews_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0mnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_groupby_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'numeric_only'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_agg_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1307\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGroupByError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[1;32m   3970\u001b[0m                             min_count=-1):\n\u001b[1;32m   3971\u001b[0m         new_items, new_blocks = self._cython_agg_blocks(\n\u001b[0;32m-> 3972\u001b[0;31m             how, alt=alt, numeric_only=numeric_only, min_count=min_count)\n\u001b[0m\u001b[1;32m   3973\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_agged_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_cython_agg_blocks\u001b[0;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[1;32m   4042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4044\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No numeric types to aggregate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4046\u001b[0m         \u001b[0;31m# reset the locs in the blocks to correspond to our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataError\u001b[0m: No numeric types to aggregate"
     ]
    }
   ],
   "source": [
    "c=news_stock_analysis(news_result, stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.dates\n",
    "def compare_plot(c, data1, data2):\n",
    "    df=pd.DataFrame({'Date':matplotlib.dates.datestr2num(c['Date']),\n",
    "                     'news_compound':np.array(c[data1]),\n",
    "                     'stock_logReturn':np.array(c[data2])})\n",
    "    plt.plot('Date',data1,data=df,marker='', color='olive', linewidth=2)\n",
    "    plt.plot('Date',data2,data=df,marker='', color='blue', linewidth=2, linestyle='dashed')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(stock_code):\n",
    "    sentiment_com_tr = []\n",
    "    sentiment_pos_tr = []\n",
    "    sentiment_neg_tr = []\n",
    "    sentiment_com_te = []\n",
    "    sentiment_pos_te = []\n",
    "    sentiment_neg_te = []\n",
    "    for i in stock_code:\n",
    "        results_page = scrolling_down_page(i)\n",
    "        all_links = get_article_link(results_page)\n",
    "        artical_content = get_article_content(all_links)\n",
    "        texts = artical_format(artical_content)\n",
    "        a = vader_comparison(texts)\n",
    "        a['Name'] = pd.to_datetime(a['Name'])\n",
    "        a = a.sort_values(['Name'],ascending = 0)\n",
    "        a_train = a[-round(len(a)*0.7):]\n",
    "        a_test = a[:round(len(a)*0.3)]\n",
    "        sentiment_com_tr.append(a_train.mean()['compound'])\n",
    "        sentiment_pos_tr.append(a_train.mean()['pos'])\n",
    "        sentiment_neg_tr.append(a_train.mean()['neg'])\n",
    "        sentiment_com_te.append(a_test.mean()['compound'])\n",
    "        sentiment_pos_te.append(a_test.mean()['pos'])\n",
    "        sentiment_neg_te.append(a_test.mean()['neg'])\n",
    "    sentiment =  [sentiment_com_tr,sentiment_pos_tr,sentiment_neg_tr,sentiment_com_te,sentiment_pos_te,sentiment_neg_te]\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = get_sentiment(stock_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.dates\n",
    "def plot_scatter(sentiment, stock_return):\n",
    "    plt.scatter(sentiment[0],stock_return, marker='x', color='m', s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(sentiment, stock_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "def plot_3D_scatter(sentiment, stock_return):\n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(111,projection='3d')\n",
    "    color_list=['r','b','g']\n",
    "    marker_list=['o','^','+']\n",
    "    z_list=list(range(1,4))\n",
    "    for i in range(3):\n",
    "        x=sentiment[i]\n",
    "        y=stock_return\n",
    "        z=[z_list[i]]*len(x)\n",
    "        ax.scatter(x,y,z,c=color_list[i],marker=marker_list[i])\n",
    "    ax.set_xlabel('Sentiment')\n",
    "    ax.set_ylabel('Stock Return')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3D_scatter(sentiment, stock_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress(sentiment):\n",
    "    X = np.column_stack((sentiment[0], sentiment[1], sentiment[2]))\n",
    "    X = sm.add_constant(X)\n",
    "    X = np.nan_to_num(X)\n",
    "    y = stock_return\n",
    "    model = sm.OLS(y,X)\n",
    "    OLS_results = model.fit()\n",
    "    return OLS_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "def reg_scatter_plot(sentiment, stock_return):\n",
    "    i=2\n",
    "    plt.scatter(sentiment[i], stock_return, s=30,c='r',marker='o',label='Sample')\n",
    "    x=sentiment[i]\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(np.array(sentiment[i]).reshape(-1,1),np.array(stock_return).reshape(-1,1))\n",
    "    a,b=regr.coef_, regr.intercept_\n",
    "    plt.plot(sorted(sentiment[i]),regr.predict(np.array(stock_return).reshape(-1,1)),color='b',linewidth=2)\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Stock Return')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_scatter_plot(sentiment, stock_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLS_result = regress(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(stock_code, OLS_result):\n",
    "    if isinstance(stock_code, str):\n",
    "        stock_code = [self.stock_code] \n",
    "        predict_result_all = []\n",
    "    for i in range(len(stock_code)):\n",
    "        predict_return = np.dot([1, sentiment[3][i],sentiment[4][i],sentiment[5][i]],OLS_results.params)\n",
    "        print('Predicted return for stock ', stock_code, ' is ', predict_return)\n",
    "        predict_result_all.append(predict_return)\n",
    "    return predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_code = 'AAPL'\n",
    "predict_result = prediction(stock_code, OLS_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
