{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Web Crawling for Stock Related News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk import sent_tokenize\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as web\n",
    "import datetime\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys  \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    '''\n",
    "    Authors and users initiate global context.\n",
    "    '''\n",
    "    def __init__(self, stock_code = None, universe_condition = 'DJIA'):\n",
    "        '''\n",
    "        Users choose the stock they are going to predict as well as \n",
    "        the stock universe the prediction based on.\n",
    "        \n",
    "        Params:\n",
    "            stock_code:           stock code, the stock to predict\n",
    "                                        'AAPL' - apple; 'GOOG' - google, etc.\n",
    "            universe_conditon:  stock index, the training of estimator is \n",
    "                                        based on the Index constituent stocks.\n",
    "                                        'DJIA' - Dow Jones Industrial Average, \n",
    "                                        'S&P500' - Standard and Poor 500 Index,\n",
    "                                        'nasdaq100' -  NASDAQ-100.\n",
    "        '''\n",
    "        self.stock_code = stock_code\n",
    "        self.universe_condition = universe_condition\n",
    "        assert [universe_condition in ['DJIA', 'S&P500', 'nasdaq100']]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class news(Context):\n",
    "    def __init__(self, stock_code, universe_condition):\n",
    "        Context.__init__(self, stock_code, universe_condition)\n",
    "        \n",
    "    def _scrolling_down_page(self, stock_name):\n",
    "        '''\n",
    "        Scroll Down Page to Get More News, use chromedriver to open the web.\n",
    "        '''\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')\n",
    "        url = \"https://finance.yahoo.com/quote/\"+ str(stock_name) +\"/?p=\" + str(stock_name)\n",
    "        driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        driver.get(url)\n",
    "        time.sleep(3) \n",
    "\n",
    "        #Selenium automates browsers, scrolling down the page until contents are fully updated\n",
    "        from selenium.webdriver import ActionChains\n",
    "        for i in range(1000): \n",
    "            ActionChains(driver).key_down(Keys.DOWN).perform() \n",
    "            print(f'have finished {i} times')\n",
    "            time.sleep(1)\n",
    "\n",
    "        #Acquire page source code\n",
    "        html_ = driver.page_source.encode('utf-8')\n",
    "        results_page = BeautifulSoup(html_,'lxml')\n",
    "        return results_page\n",
    "        \n",
    "# then we combine all these into a def function\n",
    "# stock_name should be 'AAPL' & 'GOOG' cannot be 'apple' or 'google'\n",
    "    def _get_article_link(self, results_page):\n",
    "        '''\n",
    "        Get links of article about the stock from Yahoo Finance.\n",
    "        '''\n",
    "        all_links = []\n",
    "        #url = \"https://finance.yahoo.com/quote/\"+ str(self.stock_name) +\"/?p=\" + str(self.stock_name)\n",
    "        #send the request and get the response\n",
    "        #response = requests.get(url)\n",
    "        #if not response.status_code == 200:\n",
    "        #    return None\n",
    "        try:\n",
    "            # we find the href contains in the tag_h3\n",
    "            all_h3_tags = results_page.find_all('h3', {'class': \"Mb(5px)\"})\n",
    "            article_link = []\n",
    "            # try to find the href in the tag_h3\n",
    "            for tag in all_h3_tags:\n",
    "                try:\n",
    "                    article_link.append(tag.find('a').get('href'))\n",
    "                except:\n",
    "                    return None \n",
    "            for link in article_link:\n",
    "                if 'http' in link:\n",
    "                    all_links.append(link)\n",
    "                else:\n",
    "                    home_url = 'https://finance.yahoo.com'\n",
    "                    url = home_url + link\n",
    "                    all_links.append(url)\n",
    "            return all_links\n",
    "        except:\n",
    "            return None \n",
    "        \n",
    "    def _get_article_content(self, all_links):\n",
    "        '''\n",
    "        Combine the time and article into a list and construct a list.\n",
    "        '''\n",
    "        article_content = []\n",
    "        try:\n",
    "            for link in all_links:\n",
    "                contents = []\n",
    "                response = requests.get(link)\n",
    "                if not response.status_code == 200:\n",
    "                    return None\n",
    "                result_page = BeautifulSoup(response.content,'lxml')\n",
    "                time_tag = result_page.find('time')\n",
    "                time = time_tag.get_text()\n",
    "                contents.append(time)\n",
    "                try:\n",
    "                    content_tag = result_page.find_all('p',{'class':'canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm'})\n",
    "                    for content in content_tag:\n",
    "                        article = content.get_text()\n",
    "                        contents.append(article)\n",
    "                except:\n",
    "                    return None\n",
    "                article_content.append(contents)  \n",
    "                return article_content\n",
    "        except:\n",
    "            return article_content\n",
    "        \n",
    "    def _artical_format(self, artical_content):\n",
    "        '''\n",
    "        change artical content format for text mining\n",
    "        '''\n",
    "        artical_texts = []\n",
    "        for i in artical_content:\n",
    "            if len(i)>1:\n",
    "                artical_texts.append([i[0]]+[''.join(i[1:])])\n",
    "        return artical_texts\n",
    "\n",
    "    def web_crawling(self, stock_name):\n",
    "        results_page = self._scrolling_down_page(stock_name)\n",
    "        all_links = self._get_article_link(results_page)\n",
    "        article_content = self._get_article_content(all_links)\n",
    "        artical_texts = self._artical_format(artical_content)\n",
    "        print('Web crawling for', stock_name, 'done!')\n",
    "        returm artical_texts\n",
    "        \n",
    "    \n",
    "    \n",
    "    def vader_comparison(self):\n",
    "        self._artical_format()\n",
    "        headers = ['Name','pos','neg','neu','compound']\n",
    "        self.sentiment_result=pd.DataFrame(columns=headers)\n",
    "        print(\"Name\\t\",'  pos\\t','neg\\t','neu\\t','compound')\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        for i in range(len(texts)):\n",
    "            name = self.artical_texts[i][0]\n",
    "            sentences = sent_tokenize(self.artical_texts[i][1])\n",
    "            pos=compound=neu=neg=0\n",
    "            for sentence in sentences:\n",
    "                vs = analyzer.polarity_scores(sentence)\n",
    "                pos+=vs['pos']/(len(sentences))\n",
    "                neu+=vs['neu']/(len(sentences))\n",
    "                neg+=vs['neg']/(len(sentences))\n",
    "                compound+=vs['compound']/(len(sentences))\n",
    "            self.sentiment_result=self.sentiment_result.append(pd.DataFrame([[name,pos,neg,neu,compound]],columns=headers))\n",
    "            #print('%-10s'%name,'%1.2f\\t'%pos,'%1.2f\\t'%neg,'%1.2f\\t'%neu,'%1.2f\\t'%compound)\n",
    "        self.sentiment_result=self.sentiment_result.sort_values(by='Name')\n",
    "        self.sentiment_result.index=range(len(self.sentiment_result))\n",
    "        return self.sentiment_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_article_content(all_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scrolling Down Page to Get More New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /usr/local/lib/python3.6/site-packages (18.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
      "\u001b[K    100% |################################| 911kB 10.7MB/s ta 0:00:01    99% |############################### | 901kB 44.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/site-packages (from selenium) (1.24.1)\n",
      "Installing collected packages: selenium\n",
      "Successfully installed selenium-3.141.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys  \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrolling_down_page(stock_name):\n",
    "    #Using chromedriver to open the web\n",
    "    url = \"https://finance.yahoo.com/quote/\"+ str(stock_name) +\"/?p=\" + str(stock_name)\n",
    "    driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "    time.sleep(3) \n",
    "    \n",
    "    #Selenium automates browsers, scrolling down the page until contents are fully updated\n",
    "    from selenium.webdriver import ActionChains\n",
    "    for i in range(1000): \n",
    "        ActionChains(driver).key_down(Keys.DOWN).perform() \n",
    "        print(f'have finished {i} times')\n",
    "        time.sleep(1)\n",
    "    \n",
    "    #Acquire page source code\n",
    "    html_ = driver.page_source.encode('utf-8')\n",
    "    results_page = BeautifulSoup(html_,'lxml')\n",
    "    \n",
    "    return results_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change artical content format for text mining\n",
    "def artical_format(artical_content):\n",
    "    artical_texts = []\n",
    "    for i in artical_content:\n",
    "        if len(i)>1:\n",
    "            artical_texts.append([i[0]]+[''.join(i[1:])])\n",
    "    return artical_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return text analysis results\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk import sent_tokenize\n",
    "import pandas as pd\n",
    "def vader_comparison(texts):\n",
    "    headers = ['Name','pos','neg','neu','compound']\n",
    "    result=pd.DataFrame(columns=headers)\n",
    "    print(\"Name\\t\",'  pos\\t','neg\\t','neu\\t','compound')\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for i in range(len(texts)):\n",
    "        name = texts[i][0]\n",
    "        sentences = sent_tokenize(texts[i][1])\n",
    "        pos=compound=neu=neg=0\n",
    "        for sentence in sentences:\n",
    "            vs = analyzer.polarity_scores(sentence)\n",
    "            pos+=vs['pos']/(len(sentences))\n",
    "            neu+=vs['neu']/(len(sentences))\n",
    "            neg+=vs['neg']/(len(sentences))\n",
    "            compound+=vs['compound']/(len(sentences))\n",
    "        result=result.append(pd.DataFrame([[name,pos,neg,neu,compound]],columns=headers))\n",
    "        print('%-10s'%name,'%1.2f\\t'%pos,'%1.2f\\t'%neg,'%1.2f\\t'%neu,'%1.2f\\t'%compound)\n",
    "    result=result.sort_values(by='Name')\n",
    "    result.index=range(len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = 'AAPL'\n",
    "all_links=get_article_link(stock_name)\n",
    "artical_content=get_article_content(all_links)\n",
    "artical_texts=artical_format(artical_content)\n",
    "news_result=vader_comparison(artical_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Acquiring Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stock(Context):\n",
    "    def __init__(self, stock_code, universe_condition):\n",
    "        Context.__init__(self, stock_code, universe_condition)\n",
    "\n",
    "    def _get_djia_return(self):\n",
    "        djia = requests.get('https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average')\n",
    "        soup = BeautifulSoup(djia.text, 'lxml')\n",
    "        table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "        self.codes = []\n",
    "        for row in table.findAll('tr')[1:]:\n",
    "            code = row.findAll('td')[2].text\n",
    "            self.codes.append(code)\n",
    "    \n",
    "    def _get_sp500_codes(self):\n",
    "        sp500 = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "        soup = BeautifulSoup(sp500.text, 'lxml')\n",
    "        table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "        self.codes = []\n",
    "        for row in table.findAll('tr')[1:]:\n",
    "            code = row.findAll('td')[0].text\n",
    "            self.codes.append(code)\n",
    "        \n",
    "    def _get_nasdaq100_codes(self):\n",
    "        nasdaq = requests.get('https://en.wikipedia.org/wiki/NASDAQ-100')\n",
    "        soup = BeautifulSoup(nasdaq.text, 'lxml')\n",
    "        table = soup.find('div', class_='div-col columns column-width')\n",
    "        self.codes = []\n",
    "        pattern = r'\\(([A-Z]+)\\)'\n",
    "        for row in table.findAll('li'):\n",
    "            a = re.findall(pattern, str(row.text))\n",
    "            self.codes.append(a[0])\n",
    "    \n",
    "    def get_stock_return(self):\n",
    "        start=datetime.datetime.today() - datetime.timedelta(3)\n",
    "        end=datetime.datetime.today()\n",
    "        self.ret_all = []\n",
    "        if self.universe_condition == 'DJIA':\n",
    "            self._get_djia_return()\n",
    "        elif self.universe_condition == 'S&P500':\n",
    "            self._get_sp500_codes()\n",
    "        elif self.universe_condition == 'nasdaq':\n",
    "            self._get_nasdaq100_codes()\n",
    "        for i in self.codes:\n",
    "            df = web.DataReader(i, 'yahoo', start, end)\n",
    "            ma1 = df['Close'].pct_change() \n",
    "            ret = ma1[-1]\n",
    "            self.ret_all.append(ret)\n",
    "        %matplotlib inline\n",
    "        return self.ret_all, ma1.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class news_stock(news, stock):\n",
    "    def __init__(self, stock_code, universe_condition):\n",
    "        Context.__init__(self, stock_code, universe_condition)\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        sentiment_com_tr = []\n",
    "        sentiment_pos_tr = []\n",
    "        sentiment_neg_tr = []\n",
    "        sentiment_com_te = []\n",
    "        sentiment_pos_te = []\n",
    "        sentiment_neg_te = []\n",
    "        error_code = []\n",
    "        for i in stock_code:\n",
    "            try:\n",
    "                all_links = get_article_link(i)\n",
    "                artical_content = get_article_content(all_links)\n",
    "                texts = artical_format(artical_content)\n",
    "                a = vader_comparison(texts)\n",
    "                a['Name'] = pd.to_datetime(a['Name'])\n",
    "                a = a.sort_values(['Name'],ascending = 0)\n",
    "                a_train = a[-round(len(a)*0.7):]\n",
    "                a_test = a[:round(len(a)*0.3)]\n",
    "                sentiment_com_tr.append(a_train.mean()['compound'])\n",
    "                sentiment_pos_tr.append(a_train.mean()['pos'])\n",
    "                sentiment_neg_tr.append(a_train.mean()['neg'])\n",
    "                sentiment_com_te.append(a_test.mean()['compound'])\n",
    "                sentiment_pos_te.append(a_test.mean()['pos'])\n",
    "                sentiment_neg_te.append(a_test.mean()['neg'])\n",
    "            except:\n",
    "                error_code.append(i)\n",
    "        return [sentiment_com_tr,sentiment_pos_tr,sentiment_neg_tr,sentiment_com_te,sentiment_pos_te,sentiment_neg_te,error_code]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_stock_return(stock_name):\n",
    "    start=datetime.datetime(2007, 1, 1)\n",
    "    end=datetime.datetime.today()\n",
    "    print(start,end)\n",
    "    \n",
    "    #Get Stock data from Yahoo Finance\n",
    "    df = web.DataReader('stock_name', 'yahoo', start, end)\n",
    "    df.describe() #Get summary statistics\n",
    "    \n",
    "    #Calculate percent changes\n",
    "    ma1 = df['Close'].pct_change() \n",
    "    \n",
    "    %matplotlib inline\n",
    "    return ma1.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data as web\n",
    "from datetime import datetime\n",
    "import math\n",
    "def stock_analysis(start, end, stock_name):\n",
    "    print(start,end)\n",
    "    stock_df = web.DataReader(stock_name, 'yahoo', start, end)\n",
    "    stock_df['logReturn']=pd.Series([math.log(i) for i in stock_df['Close']/stock_df['Open']],\n",
    "                                 index=stock_df.index)\n",
    "    stock_df.describe() #Get summary statistics\n",
    "    return stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=datetime(2000, 1, 1)\n",
    "end=datetime.today()\n",
    "stock_data=stock_analysis(start, end, stock_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def news_stock_analysis(news_result, stock_data):\n",
    "    headers = ['Name','pos','neg','neu','compound']\n",
    "    news_result=news_result.groupby(['Name'])[headers[1:]].mean()\n",
    "    news_result['Date']=[str(datetime.strptime(i,'%B %d, %Y'))[:10] for i in news_result.index]\n",
    "    news_result.index=range(len(news_result))\n",
    "    \n",
    "    stock_data['Date']=[str(i)[:10] for i in stock_data.index]\n",
    "    stock_data.index=range(len(stock_data))\n",
    "    \n",
    "    compound_data=news_result.merge(stock_data, on='Date', how='left')\n",
    "    return compound_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=news_stock_analysis(news_result, stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.dates\n",
    "def compare_plot(c, data1, data2):\n",
    "    df=pd.DataFrame({'Date':matplotlib.dates.datestr2num(c['Date']),\n",
    "                     'news_compound':np.array(c[data1]),\n",
    "                     'stock_logReturn':np.array(c[data2])})\n",
    "    plt.plot('Date',data1,data=df,marker='', color='olive', linewidth=2)\n",
    "    plt.plot('Date',data2,data=df,marker='', color='blue', linewidth=2, linestyle='dashed')\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
