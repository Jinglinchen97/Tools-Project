{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk import sent_tokenize\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as web\n",
    "import datetime\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys  \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    '''\n",
    "    Authors and users initiate global context.\n",
    "    '''\n",
    "    def __init__(self, stock_code = None, universe_condition = 'DJIA'):\n",
    "        '''\n",
    "        Users choose the stock they are going to predict as well as \n",
    "        the stock universe the prediction based on.\n",
    "        \n",
    "        Params:\n",
    "            stock_code:           stock code, the stock to predict\n",
    "                                        'AAPL' - apple; 'GOOG' - google, etc.\n",
    "            universe_conditon:  stock index, the training of estimator is \n",
    "                                        based on the Index constituent stocks.\n",
    "                                        'DJIA' - Dow Jones Industrial Average, \n",
    "                                        'S&P500' - Standard and Poor 500 Index,\n",
    "                                        'nasdaq100' -  NASDAQ-100.\n",
    "        '''\n",
    "        self.stock_code = stock_code\n",
    "        self.universe_condition = universe_condition\n",
    "        assert universe_condition in ['DJIA', 'S&P500', 'nasdaq100']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class news(Context):\n",
    "    def __init__(self, stock_code, universe_condition):\n",
    "        Context.__init__(self, stock_code, universe_condition)\n",
    "        \n",
    "    def _scrolling_down_page(self, stock_name):\n",
    "        '''\n",
    "        Scroll Down Page to Get More News, use chromedriver to open the web.\n",
    "        '''\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')\n",
    "        url = \"https://finance.yahoo.com/quote/\"+ str(stock_name) +\"/?p=\" + str(stock_name)\n",
    "        driver = webdriver.Chrome(executable_path = '/Users/zhaonian/downloads/chromedriver' ,chrome_options=chrome_options)\n",
    "        driver.get(url)\n",
    "        time.sleep(3) \n",
    "\n",
    "        #Selenium automates browsers, scrolling down the page until contents are fully updated\n",
    "        from selenium.webdriver import ActionChains\n",
    "        for i in range(1000): \n",
    "            ActionChains(driver).key_down(Keys.DOWN).perform() \n",
    "            print(f'have finished {i} times')\n",
    "            time.sleep(1)\n",
    "\n",
    "        #Acquire page source code\n",
    "        html_ = driver.page_source.encode('utf-8')\n",
    "        results_page = BeautifulSoup(html_,'lxml')\n",
    "        return results_page\n",
    "        \n",
    "# then we combine all these into a def function\n",
    "# stock_name should be 'AAPL' & 'GOOG' cannot be 'apple' or 'google'\n",
    "    def _get_article_link(self, results_page):\n",
    "        '''\n",
    "        Get links of article about the stock from Yahoo Finance.\n",
    "        '''\n",
    "        all_links = []\n",
    "        try:\n",
    "            # we find the href contains in the tag_h3\n",
    "            all_h3_tags = results_page.find_all('h3', {'class': \"Mb(5px)\"})\n",
    "            article_link = []\n",
    "            # try to find the href in the tag_h3\n",
    "            for tag in all_h3_tags:\n",
    "                try:\n",
    "                    article_link.append(tag.find('a').get('href'))\n",
    "                except:\n",
    "                    return None \n",
    "            for link in article_link:\n",
    "                if 'http' in link:\n",
    "                    all_links.append(link)\n",
    "                else:\n",
    "                    home_url = 'https://finance.yahoo.com'\n",
    "                    url = home_url + link\n",
    "                    all_links.append(url)\n",
    "            return all_links\n",
    "        except:\n",
    "            return None \n",
    "        \n",
    "    def _get_article_content(self, all_links):\n",
    "        '''\n",
    "        Combine the time and article into a list and construct a list.\n",
    "        '''\n",
    "        article_content = []\n",
    "        try:\n",
    "            for link in all_links:\n",
    "                contents = []\n",
    "                response = requests.get(link)\n",
    "                if not response.status_code == 200:\n",
    "                    return None\n",
    "                result_page = BeautifulSoup(response.content,'lxml')\n",
    "                time_tag = result_page.find('time')\n",
    "                time = time_tag.get_text()\n",
    "                contents.append(time)\n",
    "                try:\n",
    "                    content_tag = result_page.find_all('p',{'class':'canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm'})\n",
    "                    for content in content_tag:\n",
    "                        article = content.get_text()\n",
    "                        contents.append(article)\n",
    "                except:\n",
    "                    return None\n",
    "                article_content.append(contents)  \n",
    "                return article_content\n",
    "        except:\n",
    "            return article_content\n",
    "        \n",
    "    def _article_format(self, article_content):\n",
    "        '''\n",
    "        change article content format for text mining\n",
    "        '''\n",
    "        article_texts = []\n",
    "        for i in article_content:\n",
    "            if len(i)>1:\n",
    "                article_texts.append([i[0]]+[''.join(i[1:])])\n",
    "        return article_texts\n",
    "\n",
    "    def web_crawling(self, stock_name):\n",
    "        results_page = self._scrolling_down_page(stock_name)\n",
    "        all_links = self._get_article_link(results_page)\n",
    "        article_content = self._get_article_content(all_links)\n",
    "        article_texts = self._article_format(article_content)\n",
    "        print('Web crawling for', stock_name, 'done!')\n",
    "        return article_texts\n",
    "\n",
    "    def vader_comparison(self, article_texts):\n",
    "        headers = ['Name','pos','neg','neu','compound']\n",
    "        sentiment_result=pd.DataFrame(columns=headers)\n",
    "        #print(\"Name\\t\",'  pos\\t','neg\\t','neu\\t','compound')\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        for i in range(len(article_texts)):\n",
    "            name = article_texts[i][0]\n",
    "            sentences = sent_tokenize(article_texts[i][1])\n",
    "            pos=compound=neu=neg=0\n",
    "            for sentence in sentences:\n",
    "                vs = analyzer.polarity_scores(sentence)\n",
    "                pos+=vs['pos']/(len(sentences))\n",
    "                neu+=vs['neu']/(len(sentences))\n",
    "                neg+=vs['neg']/(len(sentences))\n",
    "                compound+=vs['compound']/(len(sentences))\n",
    "            sentiment_result=sentiment_result.append(pd.DataFrame([[name,pos,neg,neu,compound]],columns=headers))\n",
    "            #print('%-10s'%name,'%1.2f\\t'%pos,'%1.2f\\t'%neg,'%1.2f\\t'%neu,'%1.2f\\t'%compound)\n",
    "        sentiment_result=sentiment_result.sort_values(by='Name')\n",
    "        sentiment_result.index=range(len(sentiment_result))\n",
    "        return sentiment_result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stock(Context):\n",
    "    def __init__(self, stock_code, universe_condition):\n",
    "        Context.__init__(self, stock_code, universe_condition)\n",
    "\n",
    "    def _get_djia_return(self):\n",
    "        djia = requests.get('https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average')\n",
    "        soup = BeautifulSoup(djia.text, 'lxml')\n",
    "        table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "        self.codes = []\n",
    "        for row in table.findAll('tr')[1:]:\n",
    "            code = row.findAll('td')[2].text\n",
    "            self.codes.append(code[:-1])\n",
    "    \n",
    "    def _get_sp500_codes(self):\n",
    "        sp500 = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "        soup = BeautifulSoup(sp500.text, 'lxml')\n",
    "        table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "        self.codes = []\n",
    "        for row in table.findAll('tr')[1:]:\n",
    "            code = row.findAll('td')[0].text\n",
    "            self.codes.append(code)\n",
    "        \n",
    "    def _get_nasdaq100_codes(self):\n",
    "        nasdaq = requests.get('https://en.wikipedia.org/wiki/NASDAQ-100')\n",
    "        soup = BeautifulSoup(nasdaq.text, 'lxml')\n",
    "        table = soup.find('div', class_='div-col columns column-width')\n",
    "        self.codes = []\n",
    "        pattern = r'\\(([A-Z]+)\\)'\n",
    "        for row in table.findAll('li'):\n",
    "            a = re.findall(pattern, str(row.text))\n",
    "            self.codes.append(a[0][:-1])\n",
    "    \n",
    "    def get_stock_return(self):\n",
    "        start=datetime.datetime.today() - datetime.timedelta(4)\n",
    "        end=datetime.datetime.today()\n",
    "        self.ret_all = []\n",
    "        if self.universe_condition == 'DJIA':\n",
    "            self._get_djia_return()\n",
    "        elif self.universe_condition == 'S&P500':\n",
    "            self._get_sp500_codes()\n",
    "        elif self.universe_condition == 'nasdaq100':\n",
    "            self._get_nasdaq100_codes()\n",
    "        for i in self.codes:\n",
    "            df = web.DataReader(i, 'yahoo', start, end)\n",
    "            ret = df['Close'].pct_change()[-1]\n",
    "            self.ret_all.append(ret)\n",
    "        #%matplotlib inline\n",
    "        #return self.ret_all#, ma1.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class news_stock(news, stock):\n",
    "    def __init__(self, stock_code, universe_condition):\n",
    "        Context.__init__(self, stock_code, universe_condition)\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        sentiment_com_tr = []\n",
    "        sentiment_pos_tr = []\n",
    "        sentiment_neg_tr = []\n",
    "        sentiment_com_te = []\n",
    "        sentiment_pos_te = []\n",
    "        sentiment_neg_te = []\n",
    "        error_code = []\n",
    "        for i in self.codes:\n",
    "            #try:\n",
    "            article_texts = news.web_crawling(self, i)\n",
    "            a = news.vader_comparison(self, article_texts)\n",
    "            a['Name'] = pd.to_datetime(a['Name'])\n",
    "            a = a.sort_values(['Name'],ascending = 0)\n",
    "            a = a.fillna(0)\n",
    "            a_train = a[-round(len(a)*0.7):]\n",
    "            a_test = a[:round(len(a)*0.3)]\n",
    "            sentiment_com_tr.append(a_train.mean()['compound'])\n",
    "            sentiment_pos_tr.append(a_train.mean()['pos'])\n",
    "            sentiment_neg_tr.append(a_train.mean()['neg'])\n",
    "            sentiment_com_te.append(a_test.mean()['compound'])\n",
    "            sentiment_pos_te.append(a_test.mean()['pos'])\n",
    "            sentiment_neg_te.append(a_test.mean()['neg'])\n",
    "            #except:\n",
    "             #   error_code.append(i)\n",
    "        self.sentiment = [sentiment_com_tr,sentiment_pos_tr,sentiment_neg_tr,sentiment_com_te,sentiment_pos_te,sentiment_neg_te,error_code]\n",
    "    \n",
    "    def regress(self):\n",
    "        X = np.column_stack((self.sentiment[0], self.sentiment[1], self.sentiment[2]))\n",
    "        X = sm.add_constant(X)\n",
    "        X = np.nan_to_num(X)\n",
    "        y = self.ret_all\n",
    "        model = sm.OLS(y,X)\n",
    "        self.OLS_results = model.fit()\n",
    "        #print(results.params)\n",
    "        \n",
    "    def prediction(self):\n",
    "        if isinstance(self.stock_code, str):\n",
    "            stock_code = [self.stock_code] \n",
    "        self.predict_result_all = []\n",
    "        for i in range(len(stock_code)):\n",
    "            predict_return = np.dot([1, self.sentiment[3][i],self.sentiment[4][i],self.sentiment[5][i]],self.OLS_results.params)\n",
    "            print('Predicted return for stock ', self.stock_code, ' is ', predict_return)\n",
    "            self.predict_result_all.append(predict_return)\n",
    "        return self.predict_result\n",
    "    \n",
    "    def plot_scatter(self, stock_return):\n",
    "        x=self.sentiment\n",
    "        y=stock_return\n",
    "        plt.scatter(x[0],y, marker='x', color='r', s=30)\n",
    "        plt.scatter(x[1],y, marker='o', color='r', s=20)\n",
    "        plt.scatter(x[2],y, marker='^', color='r', s=30)\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_3D_scatter(self, stock_return):\n",
    "        fig=plt.figure()\n",
    "        ax=fig.add_subplot(111,projection='3d')\n",
    "        color_list=['r','b','g']\n",
    "        marker_list=['o','^','+']\n",
    "        x_list=self.sentiment\n",
    "        z_list=list(range(1,4))\n",
    "        for i in range(3):\n",
    "            x=x_list[i]\n",
    "            y=stock_return\n",
    "            z=[z_list[i]]*len(x)\n",
    "            ax.scatter(x,y,z,c=color_list[i],marker=marker_list[i])\n",
    "        ax.set_xlabel('Sentiment')\n",
    "        ax.set_ylabel('Stock Return')\n",
    "        plt.show()\n",
    "    \n",
    "    def reg_scatter_plot(self,stock_return):    \n",
    "        f, (ax1, ax2, ax3)=plt.subplots(3, sharex=False, sharey=True)\n",
    "        \n",
    "        x=self.sentiment\n",
    "        regr1=regr2=regr3=linear_model.LinearRegression()\n",
    "        regr1.fit(np.array(x[0]).reshape(-1,1),np.array(stock_return).reshape(-1,1))\n",
    "        ax1.scatter(x[0], stock_return, s=30,c='r',marker='+',label='Sample')\n",
    "        ax1.plot(sorted(x[0]),regr1.predict(np.array(stock_return).reshape(-1,1)),color='black',linewidth=2)\n",
    "\n",
    "        regr2.fit(np.array(x[1]).reshape(-1,1),np.array(stock_return).reshape(-1,1))\n",
    "        ax2.scatter(x[1], stock_return, s=20,c='b',marker='o',label='Sample')\n",
    "        ax2.plot(sorted(x[1]),regr2.predict(np.array(stock_return).reshape(-1,1)),color='black',linewidth=2)\n",
    "\n",
    "        regr3.fit(np.array(x[2]).reshape(-1,1),np.array(stock_return).reshape(-1,1))\n",
    "        ax3.scatter(x[2], stock_return, s=30,c='g',marker='^',label='Sample')\n",
    "        ax3.plot(sorted(x[2]),regr3.predict(np.array(stock_return).reshape(-1,1)),color='black',linewidth=2)\n",
    "\n",
    "        f.subplots_adjust(hspace=0.5)\n",
    "        plt.xlabel('Sentiment')\n",
    "        plt.ylabel('Stock Return')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = news_stock('AAPL','DJIA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_return=a.get_stock_return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.get_sentiment()a.regress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.regress()\n",
    "a.prediction()\n",
    "a.plot_scatter(stock_return)\n",
    "a.plot_3D_scatter(stock_return)\n",
    "a.reg_scatter_plot(stock_return)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
